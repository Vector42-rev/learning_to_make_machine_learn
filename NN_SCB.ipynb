{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsX5AIhMx0R0pkJ5pkQYwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vector42-rev/learning_to_make_machine_learn/blob/main/NN_SCB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Vocabulary and Corpus\n",
        "vocabulary = {\n",
        "    \"this\": 50, \"is\": 45, \"a\": 60, \"test\": 35, \"sentence\": 40,\n",
        "    \"machine\": 25, \"learning\": 30, \"natural\": 20, \"language\": 22,\n",
        "    \"processing\": 18, \"deep\": 15, \"neural\": 17, \"network\": 16,\n",
        "    \"model\": 25, \"data\": 40, \"science\": 22, \"python\": 30,\n",
        "    \"programming\": 20, \"artificial\": 15, \"intelligence\": 18\n",
        "}\n",
        "\n",
        "# bigram counts\n",
        "bigram_counts = {\n",
        "    (\"this\", \"is\"): 15, (\"is\", \"a\"): 16, (\"a\", \"test\"): 14,\n",
        "    (\"machine\", \"learning\"): 10, (\"natural\", \"language\"): 8,\n",
        "    (\"deep\", \"neural\"): 6, (\"neural\", \"network\"): 7,\n",
        "    (\"artificial\", \"intelligence\"): 5, (\"data\", \"science\"): 9\n",
        "}\n",
        "\n",
        "# Feature Extraction Functions\n",
        "def generate_unigram_features(sentence, vocab):\n",
        "    \"\"\"creating a unigram feature matrix where each word maps to its frequency.\"\"\"\n",
        "    words = sentence.split()\n",
        "    seq_len = len(words)\n",
        "    features = torch.zeros(seq_len, len(vocab))\n",
        "    vocab_list = list(vocab.keys())\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        # Handle out-of-vocabulary words\n",
        "        if word in vocab:\n",
        "            features[i, vocab_list.index(word)] = vocab.get(word, 0)\n",
        "        else:\n",
        "            # Add a small default frequency for unknown words\n",
        "            features[i, -1] = 1  # Last column for unknown words\n",
        "\n",
        "    return features\n",
        "\n",
        "def generate_bigram_features(sentence, bigram_counts):\n",
        "    \"\"\"creating a bigram feature matrix from bigram counts.\"\"\"\n",
        "    words = sentence.split()\n",
        "    seq_len = len(words)\n",
        "    features = torch.zeros(seq_len, len(bigram_counts))\n",
        "    bigram_list = list(bigram_counts.keys())\n",
        "\n",
        "    for i in range(seq_len - 1):\n",
        "        bigram = (words[i], words[i + 1])\n",
        "        if bigram in bigram_list:\n",
        "            features[i, bigram_list.index(bigram)] = bigram_counts.get(bigram, 0)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Model\n",
        "class SegmentationModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SegmentationModel, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.fc(features)\n",
        "\n",
        "# Training Preparation\n",
        "def prepare_model():\n",
        "    # Vocabulary modifications\n",
        "    vocab = vocabulary.copy()\n",
        "    vocab['<UNK>'] = 1\n",
        "\n",
        "    # Calculate total feature size\n",
        "    vocab_size = len(vocab)\n",
        "    bigram_size = len(bigram_counts)\n",
        "    total_feature_size = vocab_size + bigram_size\n",
        "\n",
        "    # Model Initialization\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SegmentationModel(input_size=total_feature_size).to(device)\n",
        "\n",
        "    # Training\n",
        "    corpus = [\n",
        "        \"this is a test\",\n",
        "        \"this is a sentence\",\n",
        "        \"machine learning is good\",\n",
        "        \"natural language processing is complex\",\n",
        "        \"deep neural network model\",\n",
        "        \"artificial intelligence advances quickly\",\n",
        "        \"data science with python\"\n",
        "    ]\n",
        "\n",
        "    data = [(sentence.replace(\" \", \"\"), sentence) for sentence in corpus]\n",
        "\n",
        "    train_data = data[:5]\n",
        "\n",
        "    # Dataset and DataLoader\n",
        "    class MyDataset(Dataset):\n",
        "        def __init__(self, data, vocab, bigram_counts):\n",
        "            self.data = data\n",
        "            self.vocab = vocab\n",
        "            self.bigram_counts = bigram_counts\n",
        "            self.vocab_list = list(self.vocab.keys())\n",
        "            self.bigram_list = list(bigram_counts.keys())\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            concatenated, sentence = self.data[idx]\n",
        "            words = sentence.split()\n",
        "\n",
        "            unigram_features = generate_unigram_features(sentence, self.vocab)\n",
        "            bigram_features = generate_bigram_features(sentence, self.bigram_counts)\n",
        "\n",
        "            target = torch.ones(len(words), 1)\n",
        "\n",
        "            return {\n",
        "                \"features\": torch.cat([unigram_features, bigram_features], dim=1),\n",
        "                \"target\": target,\n",
        "                \"concatenated\": concatenated\n",
        "            }\n",
        "\n",
        "    train_dataset = MyDataset(train_data, vocab, bigram_counts)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Training Loop\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            features = batch[\"features\"].float().to(device)\n",
        "            target = batch[\"target\"].float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(features)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return model, vocab, bigram_counts, device\n",
        "\n",
        "# Segmentation Function\n",
        "def segment_sentence(sentence, model, vocab, bigram_counts, device, threshold=0.5):\n",
        "    # Remove spaces from the input sentence\n",
        "    concatenated = sentence.replace(\" \", \"\")\n",
        "\n",
        "    # Prepare model for inference\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare features\n",
        "    unigram_features = generate_unigram_features(sentence, vocab)\n",
        "    bigram_features = generate_bigram_features(sentence, bigram_counts)\n",
        "    features = torch.cat([unigram_features, bigram_features], dim=1).float().to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        probabilities = model(features).cpu().numpy().flatten()\n",
        "\n",
        "    # Determine segmentation points\n",
        "    segmentation_points = [0]\n",
        "    for i, prob in enumerate(probabilities):\n",
        "        if prob > threshold:\n",
        "            segmentation_points.append(i + 1)\n",
        "\n",
        "    # Reconstruct segmented sentence\n",
        "    segmented_words = []\n",
        "    start = 0\n",
        "    for point in segmentation_points[1:]:\n",
        "        segmented_words.append(concatenated[start:point])\n",
        "        start = point\n",
        "\n",
        "    # Add the last segment if needed\n",
        "    if start < len(concatenated):\n",
        "        segmented_words.append(concatenated[start:])\n",
        "\n",
        "    return segmented_words\n",
        "\n",
        "# Prepare the model\n",
        "trained_model, vocab, bigram_counts, device = prepare_model()\n",
        "\n",
        "# Example usage function\n",
        "def predict_segmentation(input_sentence):\n",
        "    # Segment the sentence\n",
        "    segmented_result = segment_sentence(\n",
        "        input_sentence,\n",
        "        trained_model,\n",
        "        vocab,\n",
        "        bigram_counts,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f\"Original Sentence: {input_sentence}\")\n",
        "    print(f\"Segmented Result: {segmented_result}\")\n",
        "    return segmented_result\n",
        "\n",
        "# Demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    # Test some sentences\n",
        "    test_sentences = [\n",
        "        \"thisisatest\",\n",
        "        \"artificialintelligenceadvancesquickly\",\n",
        "        \"datasciencewithpython\"\n",
        "    ]\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        predict_segmentation(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN64A1kc6QHS",
        "outputId": "04f2db36-7fff-4e38-e9fe-6905ea39d463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: thisisatest\n",
            "Segmented Result: ['t', 'hisisatest']\n",
            "Original Sentence: artificialintelligenceadvancesquickly\n",
            "Segmented Result: ['a', 'rtificialintelligenceadvancesquickly']\n",
            "Original Sentence: datasciencewithpython\n",
            "Segmented Result: ['d', 'atasciencewithpython']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS DOESNT WORK CORRECTLY\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "\n",
        "# Define the LSTM-based model\n",
        "class SegmentationLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(SegmentationLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Dataset class\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, sentences, vocab):\n",
        "        self.sentences = sentences\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        sentence_len = len(sentence)\n",
        "        input_features = torch.zeros((sentence_len, len(self.vocab)))\n",
        "\n",
        "        for i, char in enumerate(sentence):\n",
        "            if char in self.vocab:\n",
        "                input_features[i, self.vocab[char]] = 1\n",
        "            else:\n",
        "                input_features[i, self.vocab['<UNK>']] = 1  # Unknown character handling\n",
        "\n",
        "        target = torch.zeros((sentence_len, 1))\n",
        "        target[1:] = 1  # Label the position between the words as 1 for a break (adjust as needed)\n",
        "\n",
        "        return input_features, target\n",
        "\n",
        "# Prepare the vocabulary\n",
        "vocab = {char: idx for idx, char in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
        "vocab['<UNK>'] = len(vocab)  # Add a unique index for unknown characters\n",
        "\n",
        "# Training data\n",
        "corpus = [\n",
        "    \"thisisatest\",\n",
        "    \"artificialintelligenceadvancesquickly\",\n",
        "    \"datasciencewithpython\"\n",
        "]\n",
        "\n",
        "# Model initialization\n",
        "input_size = len(vocab)\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "model = SegmentationLSTM(input_size, hidden_size, num_layers)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare the dataset and dataloader\n",
        "dataset = SegmentationDataset(corpus, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):  # Reduce epochs if needed\n",
        "    model.train()\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.float().to(device)\n",
        "        targets = targets.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Prediction function\n",
        "def segment_sentence(sentence, model, vocab, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    input_features = torch.zeros((len(sentence), len(vocab)))\n",
        "    for i, char in enumerate(sentence):\n",
        "        if char in vocab:\n",
        "            input_features[i, vocab[char]] = 1\n",
        "        else:\n",
        "            input_features[i, vocab['<UNK>']] = 1\n",
        "\n",
        "    input_features = input_features.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_features).cpu().numpy().flatten()\n",
        "\n",
        "    segmentation_points = [0] + [i+1 for i, prob in enumerate(output) if prob > threshold]\n",
        "    segmented_words = []\n",
        "    start = 0\n",
        "    for point in segmentation_points[1:]:\n",
        "        segmented_words.append(sentence[start:point])\n",
        "        start = point\n",
        "\n",
        "    if start < len(sentence):\n",
        "        segmented_words.append(sentence[start:])\n",
        "\n",
        "    return segmented_words\n",
        "\n",
        "# Demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    test_sentences = [\n",
        "        \"thisisatest\",\n",
        "        \"artificialintelligenceadvancesquickly\",\n",
        "        \"datasciencewithpython\"\n",
        "    ]\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        result = segment_sentence(sentence, model, vocab, device)\n",
        "        print(f\"Original: {sentence}\")\n",
        "        print(f\"Segmented: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU-sX_DF8Eox",
        "outputId": "fbb373f2-fd15-419c-b5dd-180b3e77d40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6578748822212219\n",
            "Epoch 2, Loss: 0.6249921917915344\n",
            "Epoch 3, Loss: 0.6026002168655396\n",
            "Epoch 4, Loss: 0.5199169516563416\n",
            "Epoch 5, Loss: 0.5057605504989624\n",
            "Original: thisisatest\n",
            "Segmented: ['t', 'h', 'i', 's', 'i', 's', 'a', 't', 'e', 's', 't']\n",
            "Original: artificialintelligenceadvancesquickly\n",
            "Segmented: ['a', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', 'a', 'd', 'v', 'a', 'n', 'c', 'e', 's', 'q', 'u', 'i', 'c', 'k', 'l', 'y']\n",
            "Original: datasciencewithpython\n",
            "Segmented: ['d', 'a', 't', 'a', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'w', 'i', 't', 'h', 'p', 'y', 't', 'h', 'o', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#small corpus\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Example Vocabulary and Corpus\n",
        "vocabulary = {\"this\": 10, \"is\": 15, \"a\": 20, \"test\": 8, \"sentence\": 5}\n",
        "bigram_counts = {(\"this\", \"is\"): 5, (\"is\", \"a\"): 6, (\"a\", \"test\"): 4}\n",
        "corpus = [\"this is a test\", \"this is a sentence\"]\n",
        "\n",
        "# Create Concatenated Data\n",
        "data = [\n",
        "    (\"thisisatest\", \"this is a test\"),\n",
        "    (\"thisisasentence\", \"this is a sentence\"),\n",
        "]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data = data[:1]  # Use the first example for training\n",
        "test_data = data[1:]   # Use the second example for testing\n",
        "\n",
        "# Feature Extraction Functions\n",
        "def generate_unigram_features(sentence, vocab):\n",
        "    \"\"\"Create a unigram feature matrix where each word maps to its frequency.\"\"\"\n",
        "    words = sentence.split()\n",
        "    seq_len = len(words)\n",
        "    features = torch.zeros(seq_len, len(vocab))\n",
        "    for i, word in enumerate(words):\n",
        "        features[i, list(vocab.keys()).index(word)] = vocab.get(word, 0)  # Frequency from vocabulary\n",
        "    return features\n",
        "\n",
        "def generate_bigram_features(sentence, bigram_counts):\n",
        "    \"\"\"Create a bigram feature matrix from bigram counts.\"\"\"\n",
        "    words = sentence.split()\n",
        "    seq_len = len(words)\n",
        "    features = torch.zeros(seq_len, len(bigram_counts))\n",
        "    bigram_list = list(bigram_counts.keys())\n",
        "    for i in range(seq_len - 1):\n",
        "        bigram = (words[i], words[i + 1])\n",
        "        if bigram in bigram_list:\n",
        "            features[i, bigram_list.index(bigram)] = bigram_counts.get(bigram, 0)\n",
        "    return features\n",
        "\n",
        "# Dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, vocab, bigram_counts):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.bigram_counts = bigram_counts\n",
        "        self.vocab_list = list(vocab.keys())\n",
        "        self.bigram_list = list(bigram_counts.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        concatenated, sentence = self.data[idx]\n",
        "        words = sentence.split()\n",
        "\n",
        "        unigram_features = generate_unigram_features(sentence, self.vocab)\n",
        "        bigram_features = generate_bigram_features(sentence, self.bigram_counts)\n",
        "\n",
        "        # Prepare target as binary labels for each word in the sentence\n",
        "        target = torch.ones(len(words), 1)  # Changed to have shape [seq_len, 1]\n",
        "\n",
        "        return {\n",
        "            \"features\": torch.cat([unigram_features, bigram_features], dim=1),\n",
        "            \"target\": target,\n",
        "            \"concatenated\": concatenated\n",
        "        }\n",
        "\n",
        "# Model\n",
        "class SegmentationModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SegmentationModel, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()  # Added sigmoid directly in the model\n",
        "        )\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.fc(features)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 1\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Calculate total feature size\n",
        "vocab_size = len(vocabulary)\n",
        "bigram_size = len(bigram_counts)\n",
        "total_feature_size = vocab_size + bigram_size\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = MyDataset(train_data, vocabulary, bigram_counts)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = MyDataset(test_data, vocabulary, bigram_counts)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SegmentationModel(input_size=total_feature_size).to(device)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        features = batch[\"features\"].float().to(device)\n",
        "        target = batch[\"target\"].float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "# Testing Loop\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    total_loss = 0.0\n",
        "    for batch in test_dataloader:\n",
        "        features = batch[\"features\"].float().to(device)\n",
        "        target = batch[\"target\"].float().to(device)\n",
        "\n",
        "        output = model(features)\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Test Loss: {total_loss/len(test_dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6qrHvnf0c3Z",
        "outputId": "68dff4bf-127f-435d-ae11-dc2589105dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.7484\n",
            "Epoch [2/5], Loss: 1.7189\n",
            "Epoch [3/5], Loss: 1.6895\n",
            "Epoch [4/5], Loss: 1.6603\n",
            "Epoch [5/5], Loss: 1.6314\n",
            "Test Loss: 1.4445\n"
          ]
        }
      ]
    }
  ]
}